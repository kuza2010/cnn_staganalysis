# -*- coding: utf-8 -*-
"""test_colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16G9BNoWb56SOluLqXIwA_p27GsxYqe7y

# Mount Google Drive to the Virtual Machine
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""# Imports"""

import os
import glob
import json
import time
import random
import shutil
import numpy

# numpy
import numpy as np
# torch
import torch
import torch.nn.functional as F
from torch import nn, optim
from torchsummary import summary
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import transforms
# matplotlib
from matplotlib import image as mImage

"""# Constant

Define constants which will be use in my network
"""

COVER_FOLDER = '/cover'
STEGO_FOLDER = '/stego'

COVER_SIGNAL = 0
STEGO_SIGNAL = 1

# print output each ...
PRINT_LEARNING_STATS_RATE = 40
PRINT_BATCH_LEARNING_STATS_RATE = 50

EPOCH = 300

# batch size for data loader
# 8 cover/stego pairs
DATA_LOADER_BATCH_SIZE = 16

# optimizer hyper parameters
MOMENTUM = 0.9
LEARNING_RATE = 0.005
WEIGHT_DECAY = 0.1
LR_GAMMA = 0.2

# a number of epoch when the learning rate it decrease
DECAY_EPOCH = [50, 150, 200]


# path where the best pretrained model will be stored
MODEL_STATE_PATH = '/absolute-model-path.pt'
MODEL_STATE_FALLBACK_PATH = '/absolute-state-fallback-path'

# path to store model accuracy and losses
# you can use the date for visualizing train process by executing 'acc_and_loss.py
MODEL_ACCURASY_PATH = '/absolute-path/model-accuracy.json'
MODEL_LOSS_PATH = '/absolute-path/model-loss.json'
MODEL_ACCURASY_TMP_PATH = '/absolute-path/model-accuracy-tmp.json'
MODEL_LOSS_TMP_PATH = '/absolute-path/model-loss-tmp.json'

# path where the current progress of learning will be stored
# we use this path since 'free' tier provides up to 12 hours per day for learning
# the network.
PROGRESS_PATH = '/absolute-path/state.pt'

"""# SRM kernels"""

import numpy as np

# 30 high pass filters
filter_class_1 = [
    np.array([
        [1, 0, 0],
        [0, -1, 0],
        [0, 0, 0]
    ], dtype=np.float32),
    np.array([
        [0, 1, 0],
        [0, -1, 0],
        [0, 0, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 1],
        [0, -1, 0],
        [0, 0, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 0],
        [1, -1, 0],
        [0, 0, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 0],
        [0, -1, 1],
        [0, 0, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 0],
        [0, -1, 0],
        [1, 0, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 0],
        [0, -1, 0],
        [0, 1, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 0],
        [0, -1, 0],
        [0, 0, 1]
    ], dtype=np.float32)
]

filter_class_2 = [
    np.array([
        [1, 0, 0],
        [0, -2, 0],
        [0, 0, 1]
    ], dtype=np.float32),
    np.array([
        [0, 1, 0],
        [0, -2, 0],
        [0, 1, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 1],
        [0, -2, 0],
        [1, 0, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 0],
        [1, -2, 1],
        [0, 0, 0]
    ], dtype=np.float32),
]

filter_class_3 = [
    np.array([
        [3, 0, 0],
        [0, -3, 0],
        [0, 0, 1],
    ], dtype=np.float32),
    np.array([
        [0, 3, 0],
        [0, -3, 0],
        [0, 1, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 3],
        [0, -3, 0],
        [1, 0, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 0],
        [1, -3, 3],
        [0, 0, 0]
    ], dtype=np.float32),
    np.array([
        [1, 0, 0],
        [0, -3, 0],
        [0, 0, 3]
    ], dtype=np.float32),
    np.array([
        [0, 1, 0],
        [0, -3, 0],
        [0, 3, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 1],
        [0, -3, 0],
        [3, 0, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, 0],
        [3, -3, 1],
        [0, 0, 0],
    ], dtype=np.float32)
]

filter_edge_3x3 = [
    np.array([
        [-1, 2, -1],
        [2, -4, 2],
        [0, 0, 0]
    ], dtype=np.float32),
    np.array([
        [0, 2, -1],
        [0, -4, 2],
        [0, 2, -1]
    ], dtype=np.float32),
    np.array([
        [0, 0, 0],
        [2, -4, 2],
        [-1, 2, -1]
    ], dtype=np.float32),
    np.array([
        [-1, 2, 0],
        [2, -4, 0],
        [-1, 2, 0]
    ], dtype=np.float32),
]

filter_edge_5x5 = [
    np.array([
        [-1, 2, -2, 2, -1],
        [2, -6, 8, -6, 2],
        [-2, 8, -12, 8, -2],
        [0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0]
    ], dtype=np.float32),
    np.array([
        [0, 0, -2, 2, -1],
        [0, 0, 8, -6, 2],
        [0, 0, -12, 8, -2],
        [0, 0, 8, -6, 2],
        [0, 0, -2, 2, -1]
    ], dtype=np.float32),
    np.array([
        [0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0],
        [-2, 8, -12, 8, -2],
        [2, -6, 8, -6, 2],
        [-1, 2, -2, 2, -1]
    ], dtype=np.float32),
    np.array([
        [-1, 2, -2, 0, 0],
        [2, -6, 8, 0, 0],
        [-2, 8, -12, 0, 0],
        [2, -6, 8, 0, 0],
        [-1, 2, -2, 0, 0]
    ], dtype=np.float32),
]

square_3x3 = np.array([
    [-1, 2, -1],
    [2, -4, 2],
    [-1, 2, -1]
], dtype=np.float32)

square_5x5 = np.array([
    [-1, 2, -2, 2, -1],
    [2, -6, 8, -6, 2],
    [-2, 8, -12, 8, -2],
    [2, -6, 8, -6, 2],
    [-1, 2, -2, 2, -1]
], dtype=np.float32)

all_hpf_list = filter_class_1 + filter_class_2 + filter_class_3 + filter_edge_3x3 + filter_edge_5x5 + [square_3x3,
                                                                                                       square_5x5]

hpf_3x3_list = filter_class_1 + filter_class_2 + filter_class_3 + filter_edge_3x3 + [square_3x3]
hpf_5x5_list = filter_edge_5x5 + [square_5x5]

normalized_filter_class_2 = [hpf / 2 for hpf in filter_class_2]
normalized_filter_class_3 = [hpf / 3 for hpf in filter_class_3]
normalized_filter_edge_3x3 = [hpf / 4 for hpf in filter_edge_3x3]
normalized_square_3x3 = square_3x3 / 4
normalized_filter_edge_5x5 = [hpf / 12 for hpf in filter_edge_5x5]
normalized_square_5x5 = square_5x5 / 12

all_normalized_hpf_list = filter_class_1 + normalized_filter_class_2 + normalized_filter_class_3 + \
 normalized_filter_edge_3x3 + normalized_filter_edge_5x5 + [normalized_square_3x3, normalized_square_5x5]

normalized_hpf_3x3_list = filter_class_1 + normalized_filter_class_2 + normalized_filter_class_3 + \
                          normalized_filter_edge_3x3 + [normalized_square_3x3]
normalized_hpf_5x5_list = normalized_filter_edge_5x5 + [normalized_square_5x5]

"""# Define Network

This section consist of three parts.

## 1 HPF
"""

# Image preprocessing
# High-pass filters (HPF)
class HPF(nn.Module):
    def __init__(self):
        super(HPF, self).__init__()

        weight_3x3 = nn.Parameter(torch.Tensor(normalized_hpf_3x3_list).view(25, 1, 3, 3), requires_grad=False)
        weight_5x5 = nn.Parameter(torch.Tensor(normalized_hpf_5x5_list).view(5, 1, 5, 5), requires_grad=False)

        self.preprocess_3x3 = nn.Conv2d(1, 25, kernel_size=(3, 3), bias=False)
        with torch.no_grad():
            self.preprocess_3x3.weight = weight_3x3

        self.preprocess_5x5 = nn.Conv2d(1, 30, kernel_size=(5, 5), padding=(1, 1), bias=False)
        with torch.no_grad():
            self.preprocess_5x5.weight = weight_5x5

    def forward(self, x):
        processed3x3 = self.preprocess_3x3(x)
        processed5x5 = self.preprocess_5x5(x)

        # concatenate two tensors
        #   in:  torch.Size([2, 1,256,256])
        #   out: torch.Size([2, 30, 254, 254])
        output = torch.cat((processed3x3, processed5x5), dim=1)
        output = nn.functional.relu(output)

        return output

"""## 2 ABS"""

# Absolut value activation (ABS)
class ABS(nn.Module):
    def __init__(self):
        super(ABS, self).__init__()

    def forward(self, x):
        output = torch.abs(x)
        return output

"""## 3 CnnSteganalysis"""

class CnnSteganalysis(nn.Module):
    def __init__(self):
        super(CnnSteganalysis, self).__init__()

        # <------    Preprocessing module    ------>
        self.preprocess = HPF()

        # <------    Convolution module    ------>
        self.separable_convolution_1 = nn.Sequential(
            nn.Conv2d(30, 60, kernel_size=(3, 3), padding=(1, 1), groups=30),
            ABS(),
            nn.BatchNorm2d(60),
            nn.Conv2d(60, 30, kernel_size=(1, 1)),
            nn.BatchNorm2d(30),
            nn.ReLU()
        )
        self.separable_convolution_2 = nn.Sequential(
            nn.Conv2d(30, 60, kernel_size=(3, 3), padding=(1, 1), groups=30),
            nn.BatchNorm2d(60),
            nn.Conv2d(60, 30, kernel_size=(1, 1)),
            nn.BatchNorm2d(30),
            nn.ReLU()
        )
        self.base_block_1 = nn.Sequential(
            nn.Conv2d(30, 30, kernel_size=(3, 3), padding=(1, 1)),
            nn.BatchNorm2d(30),
            nn.ReLU(),
            nn.AvgPool2d(3, stride=2, padding=1)
        )
        self.base_block_2 = nn.Sequential(
            nn.Conv2d(30, 32, kernel_size=(3, 3), padding=(1, 1)),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.AvgPool2d(3, stride=2, padding=1)
        )
        self.base_block_3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1)),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.AvgPool2d(3, stride=2, padding=1)
        )
        self.base_block_4 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=(1, 1)),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )

        # <------    Classification module   ------>
        self.fc1 = nn.Linear(128, 256)
        self.fc2 = nn.Linear(256, 1024)
        self.fc3 = nn.Linear(1024, 2)

    def forward(self, x):
        output = self.preprocess(x)

        output = self.separable_convolution_1(output)
        output = self.separable_convolution_2(output)
        output = self.base_block_1(output)
        output = self.base_block_2(output)
        output = self.base_block_3(output)
        output = self.base_block_4(output)
        output = F.adaptive_avg_pool2d(output, (1,1))
        output = output.view(-1, 128)

        output = F.relu(self.fc1(output))
        output = F.relu(self.fc2(output))
        output = self.fc3(output)

        return output

    def info(self, input_size):
        print(self)
        print(summary(self, input_size))

"""## 4 Network factory"""

def init_weights(module):
    if isinstance(module, nn.Conv2d):
        if module.weight.requires_grad:
            nn.init.xavier_uniform_(module.weight)

            if module.bias is not None:
                nn.init.constant_(module.bias, 0)

    elif isinstance(module, nn.BatchNorm2d):
        nn.init.constant_(module.weight, 1)
        nn.init.constant_(module.bias, 0)

    elif isinstance(module, nn.Linear):
        nn.init.xavier_uniform_(module.weight)
        nn.init.constant_(module.bias, 0)


def build(m_device):
    model = CnnSteganalysis()
    model.apply(init_weights)
    model.to(m_device.device)

    return model

"""# Device provider

Define device provider to choose between `cpu` and `cuda`
"""

class DeviceProvider:

    def __init__(self):
        super().__init__()
        if torch.cuda.is_available():
            self.device_info = 'cuda:0'
            self.device = torch.device('cuda:0')
        else:
            self.device_info = 'cpu'
            self.device = torch.device('cpu')

    def provide(self):
        return self.device

    @property
    def device_info(self):
        return self.__device_info

    @device_info.setter
    def device_info(self, info):
        self.__device_info = info

"""# Define data

In this section I define data set.
It will be used to load data from mounted gdrive.


"""

class CnnSteganalysisDataset(Dataset):
    def __init__(self, _img_dir: str, _type: str, seed, transform=None):
        random.seed(seed)

        _img_dir = _img_dir if _img_dir.endswith('') else _img_dir[:-1]
        cover_dir = _img_dir + COVER_FOLDER
        stego_dir = _img_dir + STEGO_FOLDER
        image_names = os.listdir(cover_dir)
        random.shuffle(image_names)

        if len(os.listdir(cover_dir)) != len(os.listdir(stego_dir)):
            raise RuntimeError("Cover and stego folders has different size!")

        # ration train (8): test (1): validation (1)
        if _type == 'validation':
            self.image_list = image_names[:1000]
        elif _type == 'test':
            self.image_list = image_names[1000:2000]
        elif _type == 'train':
            self.image_list = image_names[2000:10000]
        else:
            raise RuntimeError('Only `validation, test, train` values available!')

        self.type = _type
        self.cover_dir = cover_dir
        self.stego_dir = stego_dir
        self.transform = transform
        self.self_name = f'\'{self.type.capitalize()} dataset\''

    def __len__(self):
        return len(self.image_list)

    def __getitem__(self, index):
        name = self.image_list[index]
        cover_path = os.path.join(self.cover_dir, name)
        stego_path = os.path.join(self.stego_dir, name)
        cover_img = mImage.imread(cover_path, 'pgm')
        stego_img = mImage.imread(stego_path, 'pgm')
        # normalizing the pixel values
        cover_data = numpy.asarray(cover_img, dtype='float32')
        stego_data = numpy.asarray(stego_img, dtype='float32')
        entry = {
            'images': numpy.stack([cover_data, stego_data]),
            'label': numpy.array([COVER_SIGNAL, STEGO_SIGNAL], dtype='int32')
        }

        if self.transform:
            entry = self.transform(entry)

        return entry

    def info(self) -> str:
        return f'Data set {self.self_name} info:\n\t' \
               f'type: {self.type}\n\t' \
               f'size: {self.__len__()}' \
               f'\n\timages: {self.image_list}\n'

"""# Define data transformation"""

class TrainTransformation():
    def __call__(self, data):
        images = data["images"]
        label = data["label"]

        # Rotation
        ang = [1, 2, 3]
        angle = random.choice(ang)
        images = np.rot90(images, angle, axes=[1, 2]).copy()

        # Flip (mirroring)
        if random.random() < 0.5:
            images = np.flip(images, axis=2).copy()

        return {
            'images': images,
            'label': label
        }


class ToTensor():
    def __call__(self, data):
        images = data["images"]
        label = data["label"]

        images = np.expand_dims(images, axis=1)
        images = images.astype(np.float32)

        new_sample = {
            'images': torch.from_numpy(images),
            'label': torch.from_numpy(label).long(),
        }

        return new_sample

"""# Average meter"""

# Average meter
# https://github.com/pytorch/examples/blob/1de2ff9338bacaaffa123d03ce53d7522d5dcc2e/imagenet/main.py#L287
class AverageMeter:
    def __init__(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.count += n
        self.sum += val * n
        self.avg = self.sum / self.count


class LossHistory:
    def __init__(self):
        self.history = []

    def __str__(self) -> str:
        return '; '.join(
            [f'iteration [#{entry["iteration"]}], loss {entry["val"]:.4f}' for entry in self.history]
        )

    def reset(self):
        self.history.clear()

    def append(self, val, iteration):
        self.history.append({'val': val, 'iteration': iteration})


class AccuracyHistory:
    def __init__(self):
        self.history = []

    def __str__(self) -> str:
        return '; '.join(
            [f'epoch [#{entry["epoch"]}], accuracy {entry["accuracy"]:.4f}' for entry in self.history]
        )

    def reset(self):
        self.history.clear()

    def append(self, epoch, accuracy):
        self.history.append({'epoch': epoch, 'accuracy': accuracy})

"""# Some util functions

In this section I describe some utility-classes for better work with cnn.

## File utils
"""

def save_model(path_to_save, model, optimizer, epoch):
    # clean VM folder
    _clear_folder(path_to_save)

    all_state = {
        'epoch': epoch,
        'model_state': model.state_dict(),
        'optim_state': optimizer.state_dict()
    }

    # save to VM
    try:
      print(f'Save model to {path_to_save}')
      torch.save(all_state, path_to_save)
      print(f'Saved successfully')
    except Exception as e:
      print(f'Failed to save model to path {path_to_save}. Reason: {e} Try to save it in fallback folder: {MODEL_STATE_FALLBACK_PATH}')
      stat = json.dumps({'state': all_state})    
      with open(MODEL_STATE_FALLBACK_PATH, 'w') as outfile:
        json.dump(stat, outfile, indent=4, sort_keys=True)
        print(f'Save model to fallback folder: {MODEL_STATE_FALLBACK_PATH} sucessfully')

def save_progress(model, optimizer, epoch):
    all_state = {
        'epoch': epoch,
        'model_state': model.state_dict(),
        'optim_state': optimizer.state_dict()
    }
    print(f'Save progress to {PROGRESS_PATH}')
    torch.save(all_state, PROGRESS_PATH)
    print(f'Saved successfully')


def save_history(accuracy_history: AccuracyHistory, loss_history: LossHistory, epoch, is_tmp=False):
    loss = json.dumps({'losses': loss_history.history, 'epoch': epoch})    
    acc = json.dumps({'accuracy': accuracy_history.history, 'epoch': epoch})

    acc_path = MODEL_ACCURASY_TMP_PATH if is_tmp else MODEL_ACCURASY_PATH
    loss_path = MODEL_LOSS_TMP_PATH if is_tmp else MODEL_LOSS_PATH

    with open(acc_path, 'w') as outfile:
        # save to GD (mounted path)
        json.dump(acc, outfile, indent=4, sort_keys=True)
        print(f'Save model loss to {acc_path}')

    with open(loss_path, 'w') as outfile:
        # save to GD (mounted path)
        json.dump(loss, outfile, indent=4, sort_keys=True)
        print(f'Save model accuracy to {loss_path}')


def _clear_folder(path):
    if os.path.exists(path):
        if os.path.isfile(path):
          try:
            os.remove(path)
            return
          except Exception as e:
            print(f'Failed to delete {file_path}. Reason: {e}')
          
        for filename in os.listdir(path):
            file_path = os.path.join(path, filename)
            try:
                if os.path.isfile(file_path):
                    os.remove(file_path)
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)
            except Exception as e:
                print(f'Failed to delete {file_path}. Reason: {e}')


def _clear_gd_folder(folder_id):
    # clean GDrive
    folder_to_store_state = drive.CreateFile({'id': folder_id})
    file_list = drive.ListFile({'q': f'"{folder_id}" in parents and trashed=false'}).GetList()
    for tmp_file in file_list:
      tmp_file.Trash()

"""## Network utils"""

def get_data_loaders(t_ds, v_ds, test_ds, batch_size,
                     train_kwargs, validation_kwargs=None, test_kwargs=None):
    if test_kwargs is None:
        test_kwargs = {}
    if validation_kwargs is None:
        validation_kwargs = {}

    train_dataloader = DataLoader(t_ds, batch_size=batch_size, shuffle=True, **train_kwargs)
    test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True, **test_kwargs)
    validation_dataloader = DataLoader(v_ds, batch_size=batch_size, shuffle=False, **validation_kwargs)

    return test_dataloader, validation_dataloader, train_dataloader


def get_data_set(m_seed, train_transforms, test_transforms, validation_transforms):
    test_data_set = CnnSteganalysisDataset('/content/your_folder_name',
                              'test', m_seed, test_transforms)
    train_data_set = CnnSteganalysisDataset('/content/your_folder_name',
                               'train', m_seed, train_transforms)
    validation_data_set = CnnSteganalysisDataset('/content/your_folder_name',
                                    'validation', m_seed, validation_transforms)

    test_data_set.info()
    validation_data_set.info()
    train_data_set.info()
    return test_data_set, validation_data_set, train_data_set


def get_learning_scheduler(optimizer, milestones, gamma):
    return optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma, verbose=True)


def get_optimizer(model, weight_decay, learning_rate, momentum):
    params_wd, params_rest = [], []
    params = model.parameters()

    for param_item in params:
        if param_item.requires_grad:
            (params_wd if param_item.dim() != 1 else params_rest).append(param_item)

    param_groups = [
        {'params': params_wd, 'weight_decay': weight_decay},
        {'params': params_wd},
        {'params': params_rest}
    ]
    optimizer = optim.SGD(param_groups, lr=learning_rate, momentum=momentum)
    return optimizer


def load_state(model, m_optimizer, path):
    epoch = 0

    if os.path.exists(path):
        try:
            print(f'Detected model state by path {path}')
            state = torch.load(path)
            
            model_state = state['model_state']
            optim_state = state['optim_state']
            epoch = state['epoch']

            model.load_state_dict(model_state)
            m_optimizer.load_state_dict(optim_state)
        except EOFError as e:
            epoch = 0
            print(f'Can not load state due to: {e}')
    else:
        print('No saved state detected...')

    print(f'Start epoch is: {epoch}')
    return epoch

"""# Train network"""

def _main_decorator(func):
    def seed_wrapper():
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)

    def device_wrapper():
        print(f'Using ***{device.device_info}*** device.')

    def wrapper():
        seed_wrapper()

        print('<===============     Start   ===============>\n')
        device_wrapper()
        func()
        print('\n<===============      End     ===============>')

    return wrapper


def _provide_data_loader_kwargs():
    return {'num_workers': 1, 'pin_memory': True}


def train_model(model, m_device, train_loader, optimizer, epoch, tlh):
    _time_meter = AverageMeter()
    _train_loss = AverageMeter()
    
    model.train()
    criterion = nn.CrossEntropyLoss()
    for batch_num, entry in enumerate(train_loader):
        batch_time = time.time()
        images, target = entry['images'], entry['label']

        shape = list(images.size())  # images.size() = [16, 2, 1, 256, 256]
        images = images.reshape(shape[0] * shape[1], *shape[2:])  # images.size() = [32, 1, 256, 256]
        target = target.reshape(-1)

        images = images.to(m_device.device)
        target = target.to(m_device.device)

        # clear gradients
        optimizer.zero_grad()
        # forward propagation
        output = model(images)
        # find the loss
        loss = criterion(output, target)
        _train_loss.update(loss.item(), images.size(0))
        # compute gradient
        loss.backward()
        # update weights
        optimizer.step()
        # compute loss

        _time_meter.update(time.time() - batch_time)

        if batch_num % PRINT_LEARNING_STATS_RATE == 0:
          print(f'Epoch: [{epoch}][{batch_num + 1}/{len(train_loader)}]\t'
                f'Mini-batch time {_time_meter.val:.3f} avg({_time_meter.avg:.3f})\t'
                f'Loss {_train_loss.val:.7f} ({_train_loss.avg:.7f})\t')

        # if batch_num % PRINT_BATCH_LEARNING_STATS_RATE == 0:
            # tlh.append(_train_loss.avg, epoch*len(train_loader) + batch_num)
    tlh.append(_train_loss.avg, epoch)


def evaluate_model(model, m_device, eval_loader, is_validation=False,
                   epoch=None, optimizer=None, best_acc=None, vah=None):
    correct = 0.0
    model.eval()

    # do evaluation
    with torch.no_grad():
        for entry in eval_loader:
            images, target = entry['images'], entry['label']

            shape = list(images.size())
            images = images.reshape(shape[0] * shape[1], *shape[2:])
            target = target.reshape(-1)

            images = images.to(m_device.device)
            target = target.to(m_device.device)

            # forward propagation
            output = model(images)
            # get predictions from the maximum value
            prediction = output.max(1, keepdim=True)
            prediction_tensor = prediction[1]
            correct += prediction_tensor.eq(target.view_as(prediction_tensor)).sum().item()

    accuracy = correct / (len(eval_loader.dataset) * 2)
    
    if is_validation:
        save_progress(model, optimizer, epoch)
        # append to accuract history
        vah.append(epoch, accuracy)
        if accuracy > best_acc:
            best_acc = accuracy
            save_model(MODEL_STATE_PATH, model, optimizer, epoch)

    print('-' * 50)
    print(f'Epoch {epoch}\t'
          f'Num of images in evaluation: {len(eval_loader.dataset)}'
          f'Eval accuracy: {accuracy:.6f}\t'
          f'Best accuracy: {best_acc}')
    print('-' * 50)

    return best_acc


@_main_decorator
def main():
    # create dataset
    test_data_set, validation_data_set, train_data_set = get_data_set(
        m_seed=seed,
        train_transforms=transforms.Compose([TrainTransformation(), ToTensor()]),
        test_transforms=transforms.Compose([ToTensor()]),
        validation_transforms=transforms.Compose([ToTensor()])
    )
    # create data loaders
    test_dataloader, validation_dataloader, train_dataloader = get_data_loaders(
        train_data_set,
        validation_data_set,
        test_data_set,
        DATA_LOADER_BATCH_SIZE,
        _provide_data_loader_kwargs())

    # create model
    model = build(device)
    # create optimizer
    optimizer = get_optimizer(model, WEIGHT_DECAY, LEARNING_RATE, MOMENTUM)
    # load prev network state
    start_epoch = load_state(model, optimizer, MODEL_STATE_PATH)
    # add learning scheduler
    scheduler = get_learning_scheduler(optimizer, DECAY_EPOCH, LR_GAMMA)

    best_accuracy = 0.0
    train_loss_history = LossHistory()
    validation_accuracy_history = AccuracyHistory()

    for epoch in range(start_epoch, EPOCH):
        # train
        train_model(model, device, train_dataloader, optimizer, epoch, train_loss_history)
        # validate
        best_accuracy = evaluate_model(model=model,
                                       m_device=device,
                                       eval_loader=validation_dataloader,
                                       is_validation=True,
                                       epoch=epoch,
                                       optimizer=optimizer,
                                       best_acc=best_accuracy,
                                       vah=validation_accuracy_history)
        scheduler.step()
        # save tmp history to file
        save_history(validation_accuracy_history, train_loss_history, epoch, True)

    # load best network parameter to test
    load_state(model, optimizer, MODEL_STATE_PATH)
    evaluate_model(model, device, test_dataloader, False)
    # save history to file
    save_history(validation_accuracy_history, train_loss_history, EPOCH)



device = DeviceProvider()
seed = random.randint(0, 1234)
print(f'Seed is: {seed}')
main()